%
\subsubsection{1st order Markov Chain Model}
We also worked on Markov model in this project. In the basic (1st) Markov Chain Model the joint probability is given by

\begin{equation}
  p(x_{1:M}) = p(x_1)p(x_2|x_1)p(x_3|x_2) ...  p(x_M|x_{M-1}) = p(x_1) \prod\limits_{m=2}^M p(x_i|x_{i-1})
\end{equation}
\paragraph{}
But when we first get our trial on this \textbf{first order Markov Chain Model}, it does not give us perspective result, because this is extremely similar to what we have done in our \textbf{(1st order) Hidden Markov Model}, as what we have stated above, instead of tokenized the words into phrases, we tried the second order model instead, for the simple reason that this will do the tokenization automatically and is much more subjective in tokenization.

\subsubsection{2nd order Markov Chain Model}
\paragraph{}
In the \textbf{second order Markov Chain Model}, the assumption on the transition probability is:

\begin{equation}
  p(x_m|x_{1:m-1}) = p(x_m|x_{m-1}, x_{m-1})
\end{equation}

So, different from that in the first order model
\begin{equation}
  p(x_{1:M}) =p(x_1, x_2)p(x_3|x_2, x_1)p(x_4|x_3, x_2) ...  p(x_M|x_{M-1}, x_{M-2}) = p(x_1, x_2) \prod\limits_{m=3}^M p(x_m|x_{m-1}, x_{m-2}))
\end{equation} 
What should be mentioned is that we do counting and normalilzation for computing the piror probabilities $p(x_1 x_2)$ and trains on the transition probabilities $p(x_m|x_{m-1})$. Since we have around three thousand words in Shakespear's Sonnet, we the number of parameters (for $p(x_1 x_2)$ and $p(x_m|x_{m-1})$) is not substantially large, so the running time for 2nd order Markov Chain Model in affordable.
