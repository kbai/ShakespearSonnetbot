%
\vspace{5pt}
\paragraph{}
The basic strategy in our model training is to train multiple models for different part of the poem. Therefore, we have the following pre-processing data manipulation.

\begin{enumerate}
	\item [\textbf{Grouping}] Shakespear's sonnets enjoy the clear rhyme scheme \textit{abab cdcd efef gg}. Moreover, lines with different rhymes have quite different sentence structure. For example, the first and third lines \textit{aa} have quite different sentence structure from the last two lines \textit{gg}. Based on this obersevation, we decide to train 6 models for these 6 parts, namely \textit{a, b, c, d, e, f, g}. Therefore, we first group all the lines in the same part of the poems and get six corpuses, namely \texttt{groupA, groupB, groupC, groupD, groupE, groupF, groupG}.
	\item [\textbf{Punctuations}] Shakespear's sonnets contain various punctuations (e.g., `` ,", `` '", `` -"). We delete all punctuations except `` -", and thus the words ``Feed'st" and ``'This" become ``Feedst" and ``This". For words with hyphen `` -", we manually delete it or replace it with empty space ``  ". There are in total 83 hyphens in \texttt{Shakespear.txt} and it is very easy to deal with hyphens manually. After this stage, we have six corpuses and every corpus contains hundreds of lines without any punctuations. In poetry generation, we took our own punctuation scheme based on the writing style of Shakespeare's and our punctuations in each line of the poem are generated \textbf{automatically} by programs. 
	\item [\textbf{Tokenization}] We tokenize the words as features and use the method \texttt{text.CountVectorizer} from \texttt{sklearn.feature\_extraction} to preprocess every corpus. In simple, \texttt{CountVectorizer} lowercases all the words and builds a dictionary between the words and the natural numbers $\mathbb{N}$. The output of this tokenization step is six corpuses with sequences of natural numbers. These six corpuses will be the input of our model-training algorithms, e.g., HHM and 2nd-order Markov model.
\end{enumerate}

To achieve better poem-generating performance, we generate each line in the reverse direction with pre-sampled rhyming ending words. We keep generating lines until we get a line with exactly 10 syllables. In order to achieve these additional goals, we have the following pre-processing data manipulation.
\begin{enumerate}
	\item [\textbf{Generating rhyming dictionary}] We use the \textit{NLTK} package and the RhymeBrain website~\url{http://rhymebrain.com/en} to build a dictionary for rhyming words. For more details, see Section~\ref{sec:rhymedict}.{\color{blue}\ NEED SOME TEXT} Briefly summarize how we are using this dictionary.
	\item [\textbf{Counting syllables in each word}] We use the \textit{NLTK} package, the \textit{PyHyphen} package and our own-written function \texttt{count\_syllables()} to count the number of syllables in each word. These three methods have their own advantages and disadvantages, and we combine them to get the most accurate syllables-counting. For more details, see Section~\ref{sec:syllablecount}.
\end{enumerate}