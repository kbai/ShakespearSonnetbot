%
\paragraph{}
Here are some aspects we aer considering in data manipulation
\begin{enumerate}
	\item [\color{blue}\textbf{Tokenization}] In data processing, we make it easy, which is to say we did not tokenize some words into phrases, but instead, we try on higher order model of Markov Chain and Hidden Markov Chain. And this is actually what happens in language as well as poetry. Let us take the phrase \textbf{'fairest creatures'} as a example, the word \textbf{'fairest'} and word \textbf{'creatures'} does have to be together all the time, by using higher order probability models, namely 2nd order Markov chain for instance, we are trying to inplement the idea that \textbf{'fairest'} and word \textbf{'creatures'} can be together with some probabilty $p$ and they do not have to be together with probability $1-p$. We think this is more natural and subjective. And this is what happens in real life.
	\item [\color{blue}\textbf{rhyme}]{\color{blue}\ NEED SOME TEXT}
	\item [\color{blue}\textbf{training backwards}]{\color{blue}\ NEED SOME TEXT}
	\item [\color{blue}\textbf{Grouping}] For the ryhme consideration, we group the Shakespeare's poetry set according to the rhyme scheme \textit{abab cdcd efef gg}, so there are six groups in total, namely \textit{a, b, c, d, e, f}. Then we train different groups seperately and generate different lines belonging to different groups. In training as well as generation, we reverse each line so we can take care of the rhyme in the last word in each line. And we generate lines in pairs with our well-built ryhme dictionary.  
\end{enumerate}