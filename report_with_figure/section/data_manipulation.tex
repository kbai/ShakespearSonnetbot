%
\paragraph{}
The basic strategy in our model training is to train multiple models for different part of the poem. Therefore, we have the following pre-processing data manipulation.

\begin{enumerate}
	\item [\textbf{Grouping}] Shakespear's sonnets enjoy the clear rhyme scheme \textit{abab cdcd efef gg}. Moreover, lines with different rhymes have quite different sentence structure. For example, the first and third lines \textit{aa} have quite different sentence structure from the last two lines \textit{gg}. Based on this obersevation, we decide to train 6 models for these 6 parts, namely \textit{a, b, c, d, e, f, g}. Therefore, we first group all the lines in the same part of the poems and get six corpuses, namely \texttt{groupA, groupB, groupC, groupD, groupE, groupF, groupG}.
	\item [\textbf{Punctuations}] Shakespear's sonnets contain various punctuations (e.g., `` ,", `` '", `` -"). We delete all punctuations except `` -", and thus the words ``Feed'st" and ``'This" become ``Feedst" and ``This". For words with hyphen `` -", we manually delete it or replace it with empty space ``  ". There are in total 83 hyphens in \texttt{Shakespear.txt} and it is very easy to deal with hyphens manually. After this stage, we have six corpuses and every corpus contains hundreds of lines without any punctuations.
	\item [\textbf{Tokenization}] We tokenize the words as features and use the method \texttt{text.CountVectorizer} from \texttt{sklearn.feature\_extraction} to preprocess every corpus. In simple, \texttt{CountVectorizer} lowercases all the words and builds a dictionary between the words and the natural numbers $\mathbb{N}$. The output of this tokenization step is six corpuses with sequences of natural numbers. These six corpuses will be the input of our model-training algorithms, e.g., HHM and 2nd-order Markov model.
\end{enumerate}

To achieve better poem-generating performance, we generate each line in the reverse direction with pre-sampled ending words that rhyme. We keep generating lines until we get a line with exactly 10 syllables. In order to achieve these additional goals, we have the following pre-processing data manipulation.
\begin{enumerate}
	\item [\textbf{Generating rhyming dictionary}] We use the \textit{NLTK} package and the RhymeBrain website~\url{http://rhymebrain.com/en} to build a rhyming dictionary for each group. With this pre-built rhyming dictionary,  we generate each line in the reverse direction with pre-sampled ending words that rhyme. For more details, see Section~\ref{sec:rhymedict}.
	\item [\textbf{Counting syllables in each word}] We use the \textit{NLTK} package, the \textit{PyHyphen} package and our own-written function \texttt{count\_syllables()} to count the number of syllables in each word. These three methods have their own advantages and disadvantages, and we combine them to get the most accurate syllables-counting. For more details, see Section~\ref{sec:syllablecount}.
\end{enumerate}