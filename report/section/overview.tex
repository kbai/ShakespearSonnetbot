\paragraph{}
In the project, we work on the \textbf{Hidden Markov Model (HHM)} and the \textbf{Markov model} for poem generations. 

Due to the clear rhyme pattern in Shakespeare's sonnets, the basic strategy in our model training is to train multiple models for different part of the poem. We tokenize the words as features in each group and run the EM algorithm with different number of hidden states to train the HMMs. We developed several improvement techniques to get better poem generating performance: we train the HMM in the reverse direction and learn rhyme dictionary to sample last words that rhyme; we generate lines with total number of syllables as close to 10 as possible. With these techniques, we are able to generate poems which honor the rhyme pattern nor the iambic pentameter in Shakespeare's sonnets, and thus sound like Shakepeare's. We apply the same improvement techniques to train the Markov models as well.

The Hidden Markov Model is interesting due to the visualization and interpretation of hidden states. Looking into the model with 5 hidden states, we are able to find a clear relationship between the position of words and hidden states they correspond to. For more information, see Section \ref{sec:visualization}.

For Markov models, we pay special attention to the \textit{2nd Order Markov Model}. In comparison with the first-order Markov model, the 2nd-order Markov model turns out to be a better and more precise model in the task of poem generation. Since there are only, in total, around 3000 different words in Shakespeare's sonnets and we we trained in groups, the number of distinct words in each group is below 1000.

As additional goals, we work on data of Spenser's poetry set and we also work on the rhyme scheme in detail.