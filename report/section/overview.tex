\paragraph{}
In the project, we mainly work on \textbf{Hidden Markov Model} and \textbf{Markov Model} for poem generations. 
\paragraph{}
When working on \textit{The Hidden Markov Model}, we mainly develop several improvement techniques in training of the model : we train verses backwards and develop rhyme dictinonary for the seeding of the last words; we trained HMM in groups and generate verses in pairs In general, we take the same strategy when we work on Markov Model as well.

The Hidden Markov Model is interesting due to the viualization and interpretation of hidden states. And in visualization and interpretation, we worked on the model with 5 hidden states and are able to find a clear relationship between the position of words and hidden states they corrspond to. For more information, see Section \ref{sec:visualization}.

\paragraph{}
In Markov Model, we pay special attention in \textit{2nd Order Markov Model}. In comparision with the first order model, the 2nd Order Markov Model appears a better and more precise model in the task of poem generation. Since there are only, in total, around 3000 different words in Shakespeares poetry set and we we trained in groups, it is shown that the number of different words in total is below 1000.

\paragraph{}
As additional goals, we work on data of Spenser's Poetry set and we also work on the rhyme scheme in detail.