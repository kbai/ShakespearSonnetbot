%
\vspace{5pt}
\subsubsection{1st order Markov Chain Model}
\vspace{5pt}
\paragraph{\textit{The relationship between HMM and original Markov Chain Model}} We see in history that Hidden Markov Model is necessary when the number of observation states (number of words in Shakespeare's Sonnets, in this case) is unavoidable large. Besides this, we need Hidden Markov Model because we are supposed to get some intuitions in the grouping of words and hidden states are bringing us information. But in the case that the number of different words is affordably large (there are around 3000 words in this case), Markov Chain proves a more sophisticated model.
\paragraph{}
With this consideration and with the purpose of generating more reasonable verse set, we also worked on \textbf{Markov Chain model} in this project. In the basic (1st order) Markov Chain Model the joint probability is given by

\begin{equation}
  p(x_{1:M}) = p(x_1)p(x_2|x_1)p(x_3|x_2) ...  p(x_M|x_{M-1}) = p(x_1) \prod\limits_{m=2}^M p(x_i|x_{i-1})
\end{equation}
\paragraph{}
But when we first get our trial on this \textbf{first order Markov Chain Model}, it does not give us perspective result, because this is extremely similar to what we have done in our \textbf{(1st order) Hidden Markov Model}, as what we have stated above, instead of tokenized the words into phrases, we tried the second order model instead, for the simple reason that this will do the tokenization automatically and is much more subjective in tokenization.

\subsubsection{2nd order Markov Chain Model}
\paragraph{}
In the \textbf{second order Markov Chain Model}, the assumption on the transition probability is:

\begin{equation}
  p(x_m|x_{1:m-1}) = p(x_m|x_{m-1}, x_{m-1})
\end{equation}

So, different from the first order model, the joint probability in the 2nd order Markov Model gives:
\begin{equation}
  p(x_{1:M}) =p(x_1, x_2)p(x_3|x_2, x_1)p(x_4|x_3, x_2) ...  p(x_M|x_{M-1}, x_{M-2}) = p(x_1, x_2) \prod\limits_{m=3}^M p(x_m|x_{m-1}, x_{m-2}))
\end{equation} 
What should be mentioned is that we do counting and normalilzation for computing the piror probabilities $p(x_i, x_j)$ and trains on the transition probabilities $p(x_m|x_{m-1})$. Since we have around three thousand words in Shakespear's Sonnet, we the number of parameters (for $p(x_i, x_j)$ and $p(x_m|x_{m-1})$) is not substantially large, so the running time for 2nd order Markov Chain Model is affordable.
