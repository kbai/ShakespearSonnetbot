%
\paragraph{}
In this project, we trained \textbf{Hidden Markov Models (HMMs)} and \textbf{2nd-order Markov Models} for poem generation.

Due to the clear rhyme pattern in Shakespeare's sonnets, the basic strategy in our model training is to train multiple models for different part of the poem. We tokenize the words as features in each group and run the EM algorithm with different number of hidden states to train the HMMs. Looking into the HHMs with 5 hidden states, we are able to find a clear relationship between the position of words and hidden states they correspond to. The speech tags of words also show a strong correlation with the 5 hidden states. Since HHMs is a probabilistic model, we are able to generate poems by the HHMs we trained. 

After examining the HHMs with different number of hidden states, we found that the more hidden states the HHM has, the more meaningful sentences it can generate. Due to this observation, we decided to also train Markov models. We tried the standard (first-order) Markov model and the second-order Markov model and found that the second-order Markov model works very well to learn short phrases and to generate meaningful sentences. However, we also noticed that sometimes the second-order Markov model just \textit{``borrows"} sentences from the training dataset. But what the 2nd order Markov Model is trying to do is \textbf{capturing the meaning}, so each verse generated by it reads comparatively more reasonable. We see, because of this, 2nd Order Markov Model will generate jointed sentences like \textbf{Men call you fair}, and \textbf{you do write}'. This \textit{``borrowing''} phenomena is mainly why we think higher order may not perform much better than the second order, in the poetry language, especially,we have usually taken 2 word as a phrase. And if the order of the model is too high, actually we may generate a whole line from the original data set, and that is not what we are expecting. 

The naive approach to train both the HHMs and the 2nd-order Markov models honors neither the rhyme pattern nor the iambic pentameter in Shakespeare's sonnets. To generate poems whose lines rhyme, we first use the \textit{NLTK} package and the \textit{RhymeBrain} website~\url{http://rhymebrain.com/en} to build a \textbf{rhyming dictionary}, and then generate each line in the reverse direction with pre-sampled ending words that rhyme. To honor the iambic pentameter, we try to generate lines with total number of syllables as close to 10 as possible. To achieve this goal, we simply keep generating lines until we get a line with exactly 10 syllables. With these additional modifications, we can finally generate Shakespeare-style poems with both the HHMs and the 2nd-order Markov models. Our framework enables us to train our models with additional texts. We include all 139 of Spenser's sonnets in our training dataset and get models with bigger dictionary and generate poems with more variations. 

Since we generate rhyming lines for each group independently, our models \textit{can't} keep a unified topic across all the 14 lines. Although every line makes some sense separately, the whole poem sounds like a drunk Shakespeare who randomly jumps from one topic to another. If we have more time on this project, we may introduce some mechanism to unify all the lines with a common topic. To achieve this goal, one possible solution is to first train a topic model from the dataset. To generate a poem, one can first choose a topic and sample key words from this topic for each line. Finally, one can generate each line with the given key words with either the HMMs or the 2nd-order Markov models. In this way, the given key words in each line can unify all the lines together. 