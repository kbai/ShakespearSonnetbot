%
\paragraph{}
In this project, we trained \textbf{Hidden Markov Models (HMMs)} and \textbf{2nd-order Markov Models} for poem generation.

Due to the clear rhyme pattern in Shakespear's sonnets, the basic strategy in our model training is to train multiple models for different part of the poem. We tokenize the words as features in each group and run the EM algorithm with different number of hidden states to train the HMMs. Looking into the HHMs with 5 hidden states, we are able to find a clear relationship between the position of words and hidden states they correspond to. The speech tags of words also show a strong correlation with the 5 hidden states. Since HHMs is a probabilistic model, we are able to generate poems by the HHMs we trained. 

After examining the HHMs with different number of hidden states, we found that the more hidden states the HHM has, the more meaningful sentences it can generate. Due to this observation, we decided to also train Markov models. We tried the standard (first-order) Markov model and the second-order Markov model and found that the second-order Markov model works very well to learn short phrases and to generate meaningful sentences. However, we also noticed that sometimes the second-order Markov model just ``borrows" sentences from the training dataset. 

The naive approach to train both the HHMs and the 2nd-order Markov models honors neither the rhyme pattern nor the iambic pentameter in Shakespear's sonnet. To generate poems whose lines rhyme, we first use the \textit{NLTK} package and the RhymeBrain website~\url{http://rhymebrain.com/en} to build a rhyming dictionary, and then generate each line in the reverse direction with pre-sampled ending words that rhyme. To honor the iambic pentameter, we try to generate lines with total number of syllables as close to 10 as possible. To achieve this goal, we simply keep generating lines until we get a line with exactly 10 syllables. With these additional modifications, we can finally generate Shakespear-style poems with both the HHMs and the 2nd-order Markov models. Our framework enables us to train our models with additional texts. We include all 139 of Spenser's sonnets in our training dataset and get models with bigger dictionary and generate poems with more variations. 

Since we generate rhyming lines for each group independently, our models can't keep a unified topic across all the 14 lines. Although every line makes some sense separately, the whole poem sounds like a drunk Shapespear who randomly jumps from one topic to another. If we have more time on this project, we may introduce some mechanism to unify all the lines with a common topic. To achieve this goal, one possible solution is to first train a topic model from the dataset. To generate a poem, one can first choose a topic and sample key words from this topic for each line. Finally, one can generate each line with the given key words with either the HMMs or the 2nd-order Markov models. In this way, the given key words in each line can unify all the lines together. 